import gymnasium as gym
from gymnasium import Env
import numpy as np
from tqdm import tqdm


def mc_policy_evaluation(
    policy: np.ndarray, env_params: dict, gamma: float, first_visit: bool = True
):
    env: Env = env_params["env"]
    nS = env.observation_space.n
    nA = env.action_space.n

    state_counter = np.zeros(nS)
    V_s = np.zeros(nS)
    n_complete_ep = 0
    pbar = tqdm(range(env_params["num_episodes"]))
    for ep in pbar:
        observation, info = env.reset()
        # Play an episode
        ep_info = {"s": [], "a": [], "r": [], "s_next": []}
        for step in range(env_params["max_iter"]):
            action = np.random.choice(nA, p=policy[observation])
            ep_info["s"].append(observation)
            ep_info["a"].append(action)
            observation, reward, terminated, truncated, info = env.step(action)
            ep_info["r"].append(reward)
            ep_info["s_next"].append(observation)

            if terminated or truncated:
                n_complete_ep += 1
                break
        pbar.set_description(f"% complete eps = {int(100 * n_complete_ep / (ep+1))}")

        # Perform an MC update on the episode
        G_t = 0
        for i, (state, reward) in enumerate(
            list(zip(ep_info["s"], ep_info["r"]))[::-1]
        ):
            G_t = reward + gamma * G_t
            if state not in ep_info["s"][i::-1] or not first_visit:
                state_counter[state] += 1
                V_s[state] = V_s[state] + (G_t - V_s[state]) / state_counter[state]

    return V_s


if __name__ == "__main__":
    # env = gym.make(
    #     "FrozenLake-v1",
    #     map_name="8x8",
    #     is_slippery=False,
    #     render_mode="rgb_array",
    # )
    # env = gym.wrappers.RecordVideo(
    #     env_base,
    #     "./outputs",
    #     episode_trigger=lambda x: x % 5 == 0,
    # )
    env = gym.make("CliffWalking-v0", render_mode="rgb_array")
    observation, info = env.reset()

    gamma = 0.9
    nS = env.observation_space.n
    nA = env.action_space.n
    policy = np.ones((nS, nA)) / nA
    env_params = {"env": env, "num_episodes": int(1e3), "max_iter": int(1e4)}
    value_func = mc_policy_evaluation(policy, env_params, gamma)

    print((1e1 * value_func).reshape(4, 12).astype(int))

    print("Done!")
